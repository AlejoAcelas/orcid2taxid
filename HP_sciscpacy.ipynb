{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scispacy spacy\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fsdmFasz9ifd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bionlp13cg_md-0.5.4.tar.gz"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hKfQkacyAHyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from scispacy.abbreviation import AbbreviationDetector\n",
        "\n",
        "nlp = spacy.load(\"en_ner_bionlp13cg_md\")\n",
        "\n",
        "# Add the abbreviation pipe to the spacy pipeline.\n",
        "nlp.add_pipe(\"abbreviation_detector\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zsCAbz3n_2C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the biomedical NER model\n",
        "nlp = spacy.load(\"en_ner_bionlp13cg_md\")\n",
        "\n",
        "# Test text with viruses and bacteria\n",
        "test_text = \"COVID-19 is caused by SARS-CoV-2. Studies also show interactions with Escherichia coli and Staphylococcus aureus.\"\n",
        "\n",
        "# Process text\n",
        "doc = nlp(test_text)\n",
        "\n",
        "# Extract entities\n",
        "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "print(\"Extracted Entities:\", entities)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XXSXDNdq95wO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import requests\n",
        "from Bio import Entrez\n",
        "import time\n",
        "import re\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Removes XML tags and extra spaces from text.\"\"\"\n",
        "    return re.sub(r\"<[^>]+>\", \" \", text).strip()\n",
        "\n",
        "\n",
        "# Load SciSpaCy model for biomedical entity recognition\n",
        "print(\"Loading SciSpaCy model...\")\n",
        "nlp = spacy.load(\"en_ner_bionlp13cg_md\")\n",
        "\n",
        "def extract_entities(text):\n",
        "    \"\"\"Extract only species-related entities (including viruses) using SciSpaCy.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    organism_labels = {\"SPECIES\", \"TAXON\", \"ORGANISM\", \"VIRUS\"}  # Allowed labels\n",
        "    entities = [ent.text for ent in doc.ents if ent.label_ in organism_labels]\n",
        "    return entities\n",
        "\n",
        "def get_papers_from_orcid(orcid: str):\n",
        "    \"\"\"Get papers for a researcher using their ORCID\"\"\"\n",
        "    url = f\"https://pub.orcid.org/v3.0/{orcid}/works\"\n",
        "    headers = {\"Accept\": \"application/json\"}\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if not response.ok:\n",
        "        print(f\"Error fetching ORCID data: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "    data = response.json()\n",
        "    papers = []\n",
        "\n",
        "    for work in data.get('group', []):\n",
        "        work_summary = work['work-summary'][0]\n",
        "\n",
        "        paper = {\n",
        "            'title': work_summary.get('title', {}).get('title', {}).get('value', 'No title'),\n",
        "            'year': work_summary.get('publication-date', {}).get('year', {}).get('value', ''),\n",
        "            'doi': None\n",
        "        }\n",
        "\n",
        "        external_ids = work_summary.get('external-ids', {}).get('external-id', [])\n",
        "        for ext_id in external_ids:\n",
        "            if ext_id['external-id-type'] == 'doi':\n",
        "                paper['doi'] = ext_id['external-id-value']\n",
        "\n",
        "        papers.append(paper)\n",
        "\n",
        "    return papers\n",
        "\n",
        "def get_abstract_from_doi(doi):\n",
        "    \"\"\"Try to get abstract using Crossref\"\"\"\n",
        "    try:\n",
        "        response = requests.get(f\"https://api.crossref.org/works/{doi}\")\n",
        "        if response.ok:\n",
        "            data = response.json()\n",
        "            return data['message'].get('abstract', '')\n",
        "    except:\n",
        "        return ''\n",
        "    return ''\n",
        "\n",
        "def get_taxid(organism_name):\n",
        "    \"\"\"Get TAXID for an organism name\"\"\"\n",
        "    try:\n",
        "        handle = Entrez.esearch(db=\"taxonomy\", term=organism_name)\n",
        "        record = Entrez.read(handle)\n",
        "        handle.close()\n",
        "\n",
        "        if record[\"Count\"] != \"0\":\n",
        "            taxid = record[\"IdList\"][0]\n",
        "\n",
        "            handle = Entrez.efetch(db=\"taxonomy\", id=taxid)\n",
        "            details = Entrez.read(handle)\n",
        "            handle.close()\n",
        "\n",
        "            if details:\n",
        "                return {\n",
        "                    'taxid': taxid,\n",
        "                    'scientific_name': details[0].get('ScientificName', ''),\n",
        "                    'rank': details[0].get('Rank', ''),\n",
        "                    'division': details[0].get('Division', '')\n",
        "                }\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting TAXID for {organism_name}: {str(e)}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tbfFUFl3ZRSE",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test entity extraction\n",
        "test_text = \"COVID-19 is caused by SARS-CoV-2. Studies also show interactions with E. coli and Staphylococcus aureus.\"\n",
        "doc = nlp(test_text)\n",
        "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "print(\"Extracted Entities:\", entities)"
      ],
      "metadata": {
        "id": "qaomoP2igh_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run analysis\n",
        "orcid = \"0000-0002-7115-407X\"  # Replace with your ORCID\n",
        "Entrez.email = \"hanna.palya@warwick.ac.uk\"  # Replace with your email\n",
        "\n",
        "# Get papers\n",
        "print(f\"Fetching papers for ORCID: {orcid}\")\n",
        "papers = get_papers_from_orcid(orcid)\n",
        "print(f\"Found {len(papers)} papers\")\n",
        "\n",
        "# Store found organisms and their TAXIDs\n",
        "found_organisms = {}\n",
        "\n",
        "# Process each paper\n",
        "for paper in papers:\n",
        "    print(f\"\\nAnalyzing paper: {paper['title']}\")\n",
        "\n",
        "    # Check text content\n",
        "    text = paper['title']\n",
        "    if paper['doi']:\n",
        "        abstract = get_abstract_from_doi(paper['doi'])\n",
        "        if abstract:\n",
        "            text += \" \" + abstract\n",
        "\n",
        "    print(f\"Text for extraction: {text}\")  # Debugging print\n",
        "\n",
        "    text = clean_text(text)  # Clean before passing to extraction\n",
        "    # Extract entities using SciSpaCy\n",
        "    entities = extract_entities(text)\n",
        "\n",
        "    print(f\"Entities found: {entities}\")  # Debugging print\n",
        "\n",
        "    for entity in entities:\n",
        "        print(f\"Found organism: {entity}\")\n",
        "\n",
        "    # Look up each entity in taxonomy database\n",
        "    for entity in entities:\n",
        "        print(f\"Found organism: {entity}\")\n",
        "        if entity not in found_organisms:\n",
        "            tax_info = get_taxid(entity)\n",
        "            if tax_info:\n",
        "                found_organisms[entity] = tax_info\n",
        "                print(f\"Found organism: {entity}\")\n",
        "                print(f\"TAXID: {tax_info['taxid']}\")\n",
        "                print(f\"Scientific name: {tax_info['scientific_name']}\")\n",
        "                print(\"-\" * 40)\n",
        "        time.sleep(0.5)  # Be nice to NCBI servers\n",
        "\n",
        "print(\"\\nSummary of all organisms found:\")\n",
        "for org_name, info in found_organisms.items():\n",
        "    print(f\"\\nOrganism: {org_name}\")\n",
        "    print(f\"TAXID: {info['taxid']}\")\n",
        "    print(f\"Scientific name: {info['scientific_name']}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Cy6_vxVPFhRC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPYA+diaetdLbZWAqULVtz3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}